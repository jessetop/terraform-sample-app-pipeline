# Lab 4: Auditing & Observability

*Terraform Day 3: Enterprise Deployment & Operations*

| | |
|---|---|
| **Course** | Terraform on AWS (300-Level) |
| **Module** | Enterprise Operations |
| **Duration** | 30 minutes |
| **Difficulty** | Advanced |
| **Prerequisites** | Labs 1-3 completed |

---

## Lab Overview

### Narrative

NovaTech's SOC 2 auditor arrives next month. During the pre-audit call, the lead auditor asks Jordan, NovaTech's Director of Platform Engineering, a pointed question: *"Can you show me exactly who deployed what infrastructure, when, and through what mechanism?"*

Jordan pauses. Over the course of today, the platform team has built an impressive enterprise Terraform workflow. They have centralized state management with locking (Lab 1), brought legacy infrastructure under Terraform control (Lab 2), and automated deployments through a CI/CD pipeline with approval gates (Lab 3). But the auditor's question exposes the final gap: **operational visibility**.

Jordan needs to demonstrate three things before the audit:

1. **A complete audit trail** -- every Terraform-driven API call is captured, showing who initiated it, what changed, and when.
2. **Pipeline vs. manual distinction** -- the ability to prove that production changes flow through the approved pipeline, not from an engineer's laptop.
3. **Ongoing operational monitoring** -- a dashboard the SOC team can review at any time to verify deployment health and compliance.

This lab closes that gap. You will query CloudTrail to trace Terraform activity back to specific pipeline executions, build reusable CloudWatch Log Insights queries for audit reporting, and deploy a CloudWatch dashboard that gives NovaTech continuous operational visibility. When the auditor arrives, Jordan will have every answer ready.

This is the final piece of NovaTech's enterprise Terraform story.

### Learning Objectives

By the end of this lab, you will be able to:

- Query CloudTrail Event History to identify Terraform-related API calls and trace them to specific pipeline executions or users
- Distinguish between pipeline-initiated infrastructure changes and manual console or CLI activity using CloudTrail event fields
- Build and save reusable CloudWatch Log Insights queries that answer common audit questions
- Deploy a CloudWatch dashboard via Terraform that provides at-a-glance operational visibility into pipeline health, state operations, and deployment activity
- Articulate how the four labs completed today form a complete enterprise Terraform workflow ready for SOC 2 compliance

---

## Architecture Overview

```
┌─────────────────────────────────────────────────────────────────────────────┐
│                        AUDIT & OBSERVABILITY LAYER                          │
│                                                                             │
│  ┌───────────────────┐   ┌───────────────────┐   ┌──────────────────────┐  │
│  │   CloudTrail       │   │  CloudWatch Logs   │   │  CloudWatch          │  │
│  │   Event History    │   │  Insights Queries  │   │  Dashboard           │  │
│  │                    │   │                    │   │                      │  │
│  │  - Who deployed?   │   │  - Terraform UA    │   │  - Build duration    │  │
│  │  - What changed?   │   │  - SSM changes     │   │  - Success/failure   │  │
│  │  - When?           │   │  - Student filter  │   │  - Pipeline status   │  │
│  │  - Pipeline or     │   │  - Saved queries   │   │  - State operations  │  │
│  │    manual?         │   │                    │   │  - Quick links       │  │
│  └────────┬──────────┘   └────────┬───────────┘   └──────────┬───────────┘  │
│           │                       │                           │              │
└───────────┼───────────────────────┼───────────────────────────┼──────────────┘
            │                       │                           │
            ▼                       ▼                           ▼
┌─────────────────────────────────────────────────────────────────────────────┐
│                    DATA SOURCES (Generated by Labs 1-3)                      │
│                                                                             │
│  ┌──────────────┐  ┌──────────────┐  ┌──────────────┐  ┌──────────────┐   │
│  │  Lab 1:       │  │  Lab 2:       │  │  Lab 3:       │  │  Lab 3:       │  │
│  │  S3 State     │  │  Import       │  │  CodeBuild    │  │  CodePipeline │  │
│  │  Operations   │  │  API Calls    │  │  Executions   │  │  Executions   │  │
│  │              │  │              │  │              │  │              │   │
│  │  DynamoDB     │  │  EC2, ELB,    │  │  5 Build      │  │  Pipeline     │  │
│  │  Lock Ops     │  │  ASG, SSM     │  │  Projects     │  │  Succeeded/   │  │
│  │              │  │  API calls    │  │              │  │  Failed       │  │
│  └──────────────┘  └──────────────┘  └──────────────┘  └──────────────┘   │
│                                                                             │
└─────────────────────────────────────────────────────────────────────────────┘
```

### Key Concepts

| Concept | Description | Why It Matters |
|---------|-------------|----------------|
| **CloudTrail** | Records every AWS API call as an event, including who made the call, from where, and what changed | Provides the immutable audit trail auditors require |
| **User Agent** | HTTP header sent with every API call; Terraform sends `Terraform/1.5.0` | Distinguishes Terraform activity from console or CLI activity |
| **Source IP Address** | Origin of the API call; `codebuild.amazonaws.com` for pipeline, your IP for manual | Proves whether a change came through the pipeline or from a laptop |
| **CloudWatch Log Insights** | SQL-like query language for searching CloudWatch log groups | Enables repeatable, saveable audit queries across large log volumes |
| **CloudWatch Dashboard** | Customizable visualization of CloudWatch metrics and text widgets | Provides at-a-glance operational visibility for SOC teams and operators |
| **userIdentity.arn** | The IAM principal that made the API call, including assumed role session name | Maps API calls back to specific CodeBuild projects and pipeline stages |

---

## Part A: CloudTrail Query Demo (10 min)

> **Context:** Every AWS API call made during Labs 1-3 has been recorded by CloudTrail. We will now trace that activity to answer the auditor's questions.

### Step 1: Navigate to CloudTrail Event History

1. Open the **AWS Management Console**
2. Navigate to **CloudTrail** (search for "CloudTrail" in the top search bar)
3. Click **Event history** in the left navigation panel

You should see a chronological list of all API calls made in your AWS account. This is where the auditor will look first.

> **Instructor Note:** CloudTrail Event History retains the last 90 days of management events at no charge.

### Step 2: Filter for Terraform Activity

Use the filter dropdowns at the top of the Event History page to narrow down to Terraform-related activity. Try each of these filters one at a time:

**Filter 1: EC2 Resource Changes**
- Set **Lookup attributes** to: `Event name`
- Set **Enter an event name** to: `RunInstances`
- Click **Apply**

This shows every EC2 instance launch in the account. Look for events where the **User name** column shows your CodeBuild role.

**Filter 2: SSM Parameter Changes**
- Set **Lookup attributes** to: `Event name`
- Set **Enter an event name** to: `PutParameter`
- Click **Apply**

This shows the SSM parameter writes from Lab 3's pipeline deployments.

**Filter 3: By Pipeline Role**
- Set **Lookup attributes** to: `User name`
- Set **Enter a user name** to: `studentXX-codebuild-terraform-role`
- Click **Apply**

> **Important:** Replace `studentXX` with your actual student ID throughout this lab.

This shows **all** API calls made by the Terraform pipeline role -- every resource Terraform created, modified, or read through the pipeline.

**Filter 4: State Operations**
- Set **Lookup attributes** to: `Event name`
- Set **Enter an event name** to: `PutObject`
- Click **Apply**

Look for events where the **Resource name** includes your state bucket name (`studentXX-terraform-state`). These are state file writes.

**Filter 5: DynamoDB Lock Operations**
- Set **Lookup attributes** to: `Event name`
- Set **Enter an event name** to: `PutItem`
- Click **Apply**

Filter visually for your lock table name. These events correspond to Terraform acquiring and releasing state locks.

### Step 3: Examine a Specific Event in Detail

Click on any **PutParameter** event from Filter 2 above to expand the event details. Click **View event** to see the full JSON record.

Here is an annotated example of what a CloudTrail event looks like for a Terraform pipeline deployment:

```json
{
    "eventVersion": "1.08",
    "userIdentity": {
        "type": "AssumedRole",
        "principalId": "AROAXXXXXXXXXXXXXXXXX:studentXX-terraform-apply-staging",
        "arn": "arn:aws:sts::123456789012:assumed-role/studentXX-codebuild-terraform-role/studentXX-terraform-apply-staging",
        "accountId": "123456789012",
        "accessKeyId": "ASIAXXXXXXXXXXXXXXXXX",
        "sessionContext": {
            "sessionIssuer": {
                "type": "Role",
                "principalId": "AROAXXXXXXXXXXXXXXXXX",
                "arn": "arn:aws:iam::123456789012:role/studentXX-codebuild-terraform-role",
                "accountId": "123456789012",
                "userName": "studentXX-codebuild-terraform-role"
            },
            "webIdFederationData": {},
            "attributes": {
                "creationDate": "2024-01-15T14:32:00Z",
                "mfaAuthenticated": "false"
            }
        }
    },
    "eventTime": "2024-01-15T14:33:45Z",
    "eventSource": "ssm.amazonaws.com",
    "eventName": "PutParameter",
    "awsRegion": "us-east-1",
    "sourceIPAddress": "codebuild.amazonaws.com",
    "userAgent": "Terraform/1.5.0 (+https://www.terraform.io) aws-sdk-go-v2/1.24.0",
    "requestParameters": {
        "name": "/studentXX/staging/app-config",
        "description": "Application configuration for staging",
        "type": "String",
        "overwrite": true,
        "tags": [
            {
                "key": "Name",
                "value": "studentXX-staging-config"
            },
            {
                "key": "Environment",
                "value": "staging"
            }
        ]
    },
    "responseElements": {
        "version": 1,
        "tier": "Standard"
    },
    "requestID": "a1b2c3d4-e5f6-7890-abcd-ef1234567890",
    "eventID": "f1e2d3c4-b5a6-7890-1234-567890abcdef",
    "readOnly": false,
    "resources": [
        {
            "accountId": "123456789012",
            "ARN": "arn:aws:ssm:us-east-1:123456789012:parameter/studentXX/staging/app-config"
        }
    ],
    "eventType": "AwsApiCall",
    "managementEvent": true,
    "recipientAccountId": "123456789012",
    "eventCategory": "Management"
}
```

**Key fields to examine (these are the fields the auditor cares about):**

| Field | Value in Example | What It Tells You |
|-------|-----------------|-------------------|
| `userIdentity.arn` | `...assumed-role/studentXX-codebuild-terraform-role/studentXX-terraform-apply-staging` | The **CodeBuild project** that made this API call. The session name (`studentXX-terraform-apply-staging`) identifies the exact pipeline stage. |
| `userIdentity.sessionContext.sessionIssuer.userName` | `studentXX-codebuild-terraform-role` | The **IAM role** assumed by CodeBuild. This is the role you created in Lab 3. |
| `userAgent` | `Terraform/1.5.0 (+https://www.terraform.io)` | Confirms this was a **Terraform operation**, not a manual AWS CLI or console action. |
| `sourceIPAddress` | `codebuild.amazonaws.com` | Confirms the call originated from **CodeBuild** (the pipeline), not from an engineer's laptop. |
| `eventTime` | `2024-01-15T14:33:45Z` | The **exact timestamp** of the API call, in UTC. |
| `eventName` | `PutParameter` | The specific **API action** performed. |
| `requestParameters.name` | `/studentXX/staging/app-config` | The **specific resource** that was created or modified. |

### Step 4: Compare Pipeline Activity vs. Console Activity

This is the critical distinction for SOC 2 compliance. Find two events to compare:

**Pipeline Activity (from Lab 3):**
- `sourceIPAddress`: `codebuild.amazonaws.com`
- `userAgent`: Contains `Terraform/1.5.0`
- `userIdentity.arn`: Contains `studentXX-codebuild-terraform-role`

**Console Activity (from Lab 1 or Lab 2 manual work):**
- `sourceIPAddress`: Your IP address (e.g., `203.0.113.50`) or `console.amazonaws.com`
- `userAgent`: Contains `console.amazonaws.com` or `aws-cli`
- `userIdentity.arn`: Contains your IAM user, not the CodeBuild role

> **Auditor's Question Answered:** "Were all production changes made through the approved pipeline?"
>
> **Your Answer:** "Yes. All production `PutParameter` and resource creation events show `sourceIPAddress: codebuild.amazonaws.com` and `userAgent: Terraform/1.5.0`, confirming they originated from the CodeBuild pipeline. Manual console activity shows a different source IP and user agent."

### Step 5: Discussion -- Tracing Back to the Pipeline Trigger

CloudTrail tells you *which CodeBuild project* made the API call, but not *who triggered the pipeline*. To complete the audit chain, you need to cross-reference:

1. **CloudTrail `eventTime`** -- note the timestamp of the API call
2. **CodePipeline Execution History** -- navigate to CodePipeline in the console, click your pipeline, and view the execution history. Match the timestamp to a specific execution.
3. **CodeCommit Commit History** -- the pipeline execution links to a specific source commit. The commit shows the author and commit message.

**The complete audit chain:**

```
Git Commit (who wrote the code)
    └──> CodePipeline Execution (when it ran)
             └──> CodeBuild Project (which stage)
                      └──> CloudTrail Event (what AWS API was called)
                               └──> AWS Resource (what changed)
```

> **Key Insight:** The combination of Git history + CodePipeline execution history + CloudTrail events gives you a complete, tamper-resistant audit trail from "who wrote the code" to "what changed in AWS."

---

## Part B: CloudWatch Log Insights (5 min)

> **Context:** CloudTrail Event History works for one-off investigations, but auditors often want bulk queries across thousands of events. CloudWatch Log Insights provides a SQL-like query language for this purpose.

### Step 6: Navigate to CloudWatch Logs Insights

1. Open **AWS Management Console**
2. Navigate to **CloudWatch** (search for "CloudWatch" in the top search bar)
3. In the left navigation, expand **Logs** and click **Logs Insights**

### Step 7: Select the CloudTrail Log Group

In the **Select log group(s)** dropdown, look for a log group named something like:

- `aws-cloudtrail-logs-123456789012-xxxxxxxx`
- `CloudTrail/DefaultLogGroup`

> **Note:** If no CloudTrail log group is available, your account may not have CloudTrail configured to send events to CloudWatch Logs. In that case, your instructor will demonstrate this section. You can still proceed to Part C.

Select the CloudTrail log group and set the time range to **Last 12 hours** (to capture all of today's lab activity).

### Step 8: Run Query for All Terraform User-Agent Activity

Paste the following query into the query editor and click **Run query**:

```sql
fields @timestamp, @message, userIdentity.arn, eventName, eventSource
| filter userAgent like /Terraform/
| sort @timestamp desc
| limit 50
```

**What this query does:**
- `fields` -- selects the columns to display
- `filter userAgent like /Terraform/` -- matches only events where the user agent string contains "Terraform" (regex match)
- `sort @timestamp desc` -- newest events first
- `limit 50` -- return the most recent 50 matches

**Expected Output:**

You should see a table of events showing every AWS API call made by Terraform during today's labs. Each row shows:
- The timestamp of the call
- The IAM role/user that made it
- The API action (e.g., `PutParameter`, `CreateBucket`, `PutObject`)
- The AWS service (e.g., `ssm.amazonaws.com`, `s3.amazonaws.com`)

```
@timestamp                  | userIdentity.arn                                              | eventName       | eventSource
2024-01-15 14:33:45.000     | arn:aws:sts::123456789012:assumed-role/studentXX-codebuild... | PutParameter    | ssm.amazonaws.com
2024-01-15 14:33:44.000     | arn:aws:sts::123456789012:assumed-role/studentXX-codebuild... | PutParameter    | ssm.amazonaws.com
2024-01-15 14:33:40.000     | arn:aws:sts::123456789012:assumed-role/studentXX-codebuild... | PutObject       | s3.amazonaws.com
2024-01-15 14:20:12.000     | arn:aws:sts::123456789012:assumed-role/studentXX-codebuild... | DescribeVpcs    | ec2.amazonaws.com
...
```

### Step 9: Run Query for Specific Resource Changes

Now run a more targeted query to find SSM parameter changes made by your student pipeline:

```sql
fields @timestamp, eventName, userIdentity.arn, requestParameters.name
| filter eventSource = "ssm.amazonaws.com"
| filter eventName in ["PutParameter", "DeleteParameter", "GetParameter"]
| filter requestParameters.name like /studentXX/
| sort @timestamp desc
| limit 20
```

> **Important:** Replace `studentXX` with your actual student ID in the query.

**What this query does:**
- Filters to only SSM API calls
- Matches only parameter create, delete, and read operations
- Filters to only parameters containing your student ID in the name
- Shows the most recent 20 matching events

**Expected Output:**

```
@timestamp                  | eventName       | userIdentity.arn                                | requestParameters.name
2024-01-15 14:33:45.000     | PutParameter    | ...assumed-role/studentXX-codebuild.../staging  | /studentXX/staging/app-config
2024-01-15 14:33:44.000     | PutParameter    | ...assumed-role/studentXX-codebuild.../prod     | /studentXX/prod/app-config
2024-01-15 12:15:30.000     | PutParameter    | ...user/studentXX                               | /studentXX/staging/demo-parameter
...
```

> **Auditor's Question Answered:** "Show me every change to production SSM parameters in the last 30 days."
>
> **Your Answer:** Run the query above, change the time range to 30 days, and filter for `requestParameters.name like /studentXX.*prod/`. Export the results as a CSV for the audit report.

### Step 10: Save the Query for Reuse

1. Click **Save** above the query editor
2. Enter a name: `studentXX-terraform-activity`
3. Optionally add a description: `Queries all Terraform-initiated API calls filtered by student ID`
4. Click **Save**

The saved query will appear in the **Saved queries** panel on the right side. Your SOC team can run this query at any time without needing to remember the syntax.

> **Tip:** You can also schedule these queries as part of a CloudWatch Logs Insights report for automated weekly audit summaries.

---

## Part C: Deploy Observability Dashboard (15 min)

> **Context:** Queries answer specific questions, but NovaTech's operations team needs an always-on dashboard that shows pipeline health at a glance. We will deploy this dashboard using Terraform -- because everything at NovaTech is managed as code now.

### Step 11: Create Working Directory

```bash
mkdir -p ~/terraform-day3/lab4-observability
cd ~/terraform-day3/lab4-observability
```

### Step 12: Create providers.tf

Create the file `providers.tf`:

```hcl
# providers.tf - Backend and provider configuration for observability dashboard
# NOTE: Use the bucket and table names from your Lab 1 terraform output

terraform {
  required_version = ">= 1.5.0"

  backend "s3" {
    bucket         = "studentXX-terraform-state-SUFFIX"   # Use actual bucket name from Lab 1 output
    key            = "observability/terraform.tfstate"
    region         = "us-east-1"
    encrypt        = true
    dynamodb_table = "studentXX-terraform-lock-SUFFIX"    # Use actual table name from Lab 1 output
  }

  required_providers {
    aws = {
      source  = "hashicorp/aws"
      version = "~> 6.0"
    }
  }
}

provider "aws" {
  region = "us-east-1"

  default_tags {
    tags = {
      Student   = "studentXX"
      Purpose   = "Terraform Observability"
      ManagedBy = "Terraform"
      Lab       = "lab4-observability"
    }
  }
}
```

Create the file `variables.tf`:

```hcl
# variables.tf
# Input variables for Lab 4 observability dashboard

variable "student_id" {
  description = "Your student ID (e.g., student01)"
  type        = string
  default     = "studentXX"
}
```

> **Important:** Replace every instance of `studentXX` with your actual student ID before proceeding.

### Step 13: Create dashboard.tf

Create the file `dashboard.tf` with the complete CloudWatch dashboard definition:

```hcl
# dashboard.tf - CloudWatch Dashboard for Terraform Operations Monitoring
#
# IMPORTANT: Replace SUFFIX in S3 bucket and DynamoDB table names with your
# actual random suffix from Lab 1 terraform output.
#
# This dashboard provides at-a-glance visibility into:
#   - CI/CD pipeline health (build duration, success/failure rates)
#   - Pipeline execution status (succeeded vs. failed)
#   - State backend operations (S3 reads/writes, DynamoDB lock activity)
#   - Quick links to key AWS console pages
#   - Audit query reference for the SOC team

resource "aws_cloudwatch_dashboard" "terraform_ops" {
  dashboard_name = "${var.student_id}-terraform-operations"

  dashboard_body = jsonencode({
    widgets = [

      # ========================================================================
      # ROW 0: Dashboard Title
      # ========================================================================
      {
        type   = "text"
        x      = 0
        y      = 0
        width  = 24
        height = 2
        properties = {
          markdown = <<-EOF
# Terraform Pipeline Operations Dashboard
**Student:** ${var.student_id} | **Account:** NovaTech AWS | **Region:** us-east-1

This dashboard provides operational visibility into Terraform CI/CD pipeline activity, state management operations, and deployment health. Use this for ongoing monitoring and SOC 2 audit evidence.
EOF
        }
      },

      # ========================================================================
      # ROW 1: CodeBuild Metrics
      # ========================================================================

      # Widget: Build Duration (all 5 projects)
      {
        type   = "metric"
        x      = 0
        y      = 2
        width  = 8
        height = 6
        properties = {
          title   = "CodeBuild Duration (seconds)"
          region  = "us-east-1"
          view    = "timeSeries"
          stacked = false
          period  = 300
          stat    = "Average"
          metrics = [
            ["AWS/CodeBuild", "Duration", "ProjectName", "${var.student_id}-terraform-validate", { label = "Validate" }],
            ["AWS/CodeBuild", "Duration", "ProjectName", "${var.student_id}-terraform-plan-staging", { label = "Plan Staging" }],
            ["AWS/CodeBuild", "Duration", "ProjectName", "${var.student_id}-terraform-apply-staging", { label = "Apply Staging" }],
            ["AWS/CodeBuild", "Duration", "ProjectName", "${var.student_id}-terraform-plan-prod", { label = "Plan Prod" }],
            ["AWS/CodeBuild", "Duration", "ProjectName", "${var.student_id}-terraform-apply-prod", { label = "Apply Prod" }]
          ]
        }
      },

      # Widget: Build Success vs Failure (staging and prod apply projects)
      {
        type   = "metric"
        x      = 8
        y      = 2
        width  = 8
        height = 6
        properties = {
          title   = "Build Success vs Failure"
          region  = "us-east-1"
          view    = "timeSeries"
          stacked = false
          period  = 3600
          stat    = "Sum"
          metrics = [
            ["AWS/CodeBuild", "SucceededBuilds", "ProjectName", "${var.student_id}-terraform-apply-staging", { label = "Staging Succeeded", color = "#2ca02c" }],
            ["AWS/CodeBuild", "FailedBuilds", "ProjectName", "${var.student_id}-terraform-apply-staging", { label = "Staging Failed", color = "#d62728" }],
            ["AWS/CodeBuild", "SucceededBuilds", "ProjectName", "${var.student_id}-terraform-apply-prod", { label = "Prod Succeeded", color = "#1f77b4" }],
            ["AWS/CodeBuild", "FailedBuilds", "ProjectName", "${var.student_id}-terraform-apply-prod", { label = "Prod Failed", color = "#ff7f0e" }]
          ]
        }
      },

      # Widget: Pipeline Execution Counts (single value)
      {
        type   = "metric"
        x      = 16
        y      = 2
        width  = 4
        height = 3
        properties = {
          title   = "Pipeline Succeeded"
          region  = "us-east-1"
          view    = "singleValue"
          period  = 86400
          stat    = "Sum"
          metrics = [
            ["AWS/CodePipeline", "PipelineExecutionSucceeded", "PipelineName", "${var.student_id}-terraform-pipeline", { label = "Succeeded", color = "#2ca02c" }]
          ]
        }
      },
      {
        type   = "metric"
        x      = 20
        y      = 2
        width  = 4
        height = 3
        properties = {
          title   = "Pipeline Failed"
          region  = "us-east-1"
          view    = "singleValue"
          period  = 86400
          stat    = "Sum"
          metrics = [
            ["AWS/CodePipeline", "PipelineExecutionFailed", "PipelineName", "${var.student_id}-terraform-pipeline", { label = "Failed", color = "#d62728" }]
          ]
        }
      },

      # Widget: Pipeline Execution Time Series
      {
        type   = "metric"
        x      = 16
        y      = 5
        width  = 8
        height = 3
        properties = {
          title   = "Pipeline Executions Over Time"
          region  = "us-east-1"
          view    = "timeSeries"
          stacked = true
          period  = 3600
          stat    = "Sum"
          metrics = [
            ["AWS/CodePipeline", "PipelineExecutionSucceeded", "PipelineName", "${var.student_id}-terraform-pipeline", { label = "Succeeded", color = "#2ca02c" }],
            ["AWS/CodePipeline", "PipelineExecutionFailed", "PipelineName", "${var.student_id}-terraform-pipeline", { label = "Failed", color = "#d62728" }]
          ]
        }
      },

      # ========================================================================
      # ROW 2: State & Infrastructure Operations
      # ========================================================================

      # Section Header
      {
        type   = "text"
        x      = 0
        y      = 8
        width  = 24
        height = 1
        properties = {
          markdown = "## State & Infrastructure Operations"
        }
      },

      # Widget: S3 State Bucket Operations
      {
        type   = "metric"
        x      = 0
        y      = 9
        width  = 12
        height = 6
        properties = {
          title   = "State Bucket Operations (S3)"
          region  = "us-east-1"
          view    = "timeSeries"
          stacked = false
          period  = 300
          stat    = "Sum"
          # NOTE: Replace SUFFIX with your actual random suffix from Lab 1
          metrics = [
            ["AWS/S3", "GetRequests", "BucketName", "${var.student_id}-terraform-state-SUFFIX", "FilterId", "EntireBucket", { label = "Get Requests (Read State)", color = "#1f77b4" }],
            ["AWS/S3", "PutRequests", "BucketName", "${var.student_id}-terraform-state-SUFFIX", "FilterId", "EntireBucket", { label = "Put Requests (Write State)", color = "#ff7f0e" }]
          ]
        }
      },

      # Widget: DynamoDB Lock Operations
      {
        type   = "metric"
        x      = 12
        y      = 9
        width  = 12
        height = 6
        properties = {
          title   = "State Lock Operations (DynamoDB)"
          region  = "us-east-1"
          view    = "timeSeries"
          stacked = false
          period  = 300
          stat    = "Sum"
          # NOTE: Replace SUFFIX with your actual random suffix from Lab 1
          metrics = [
            ["AWS/DynamoDB", "ConsumedReadCapacityUnits", "TableName", "${var.student_id}-terraform-lock-SUFFIX", { label = "Lock Reads (Check/Acquire)", color = "#2ca02c" }],
            ["AWS/DynamoDB", "ConsumedWriteCapacityUnits", "TableName", "${var.student_id}-terraform-lock-SUFFIX", { label = "Lock Writes (Acquire/Release)", color = "#9467bd" }]
          ]
        }
      },

      # ========================================================================
      # ROW 3: Quick Links & Audit Reference
      # ========================================================================

      # Section Header
      {
        type   = "text"
        x      = 0
        y      = 15
        width  = 24
        height = 1
        properties = {
          markdown = "## Quick Links & Audit Reference"
        }
      },

      # Widget: Quick Links Panel
      {
        type   = "text"
        x      = 0
        y      = 16
        width  = 12
        height = 5
        properties = {
          markdown = <<-EOF
### Console Quick Links

| Resource | Link |
|----------|------|
| **CodePipeline** | [View Pipeline](https://console.aws.amazon.com/codesuite/codepipeline/pipelines/${var.student_id}-terraform-pipeline/view?region=us-east-1) |
| **CodeBuild** | [Build Projects](https://console.aws.amazon.com/codesuite/codebuild/projects?region=us-east-1) |
| **CloudTrail** | [Event History](https://console.aws.amazon.com/cloudtrail/home?region=us-east-1#/events) |
| **S3 State Bucket** | [View Bucket](https://console.aws.amazon.com/s3/buckets/${var.student_id}-terraform-state-SUFFIX?region=us-east-1) |
| **DynamoDB Lock Table** | [View Table](https://console.aws.amazon.com/dynamodbv2/home?region=us-east-1#table?name=${var.student_id}-terraform-lock-SUFFIX) |
| **CloudWatch Logs Insights** | [Run Queries](https://console.aws.amazon.com/cloudwatch/home?region=us-east-1#logsV2:logs-insights) |
EOF
        }
      },

      # Widget: Audit Query Reference Panel
      {
        type   = "text"
        x      = 12
        y      = 16
        width  = 12
        height = 5
        properties = {
          markdown = <<-EOF
### CloudTrail Audit Queries (Log Insights)

**All Terraform Activity:**
```
fields @timestamp, eventName, userIdentity.arn
| filter userAgent like /Terraform/
| sort @timestamp desc | limit 50
```

**SSM Parameter Changes by Student:**
```
fields @timestamp, eventName, requestParameters.name
| filter eventSource = "ssm.amazonaws.com"
| filter requestParameters.name like /studentXX/
| sort @timestamp desc | limit 20
```

**Pipeline vs Manual Activity:**
```
fields @timestamp, eventName, sourceIPAddress
| filter userAgent like /Terraform/
| filter sourceIPAddress = "codebuild.amazonaws.com"
| sort @timestamp desc | limit 50
```

*In CloudTrail Event History, filter by:*
- **User name:** `${var.student_id}-codebuild-terraform-role`
- **Event source:** `ssm.amazonaws.com`, `ec2.amazonaws.com`
EOF
        }
      }
    ]
  })
}

# ============================================================================
# Outputs
# ============================================================================

output "dashboard_url" {
  description = "URL to the CloudWatch dashboard in the AWS Console"
  value       = "https://console.aws.amazon.com/cloudwatch/home?region=us-east-1#dashboards:name=${var.student_id}-terraform-operations"
}

output "dashboard_name" {
  description = "Name of the deployed CloudWatch dashboard"
  value       = aws_cloudwatch_dashboard.terraform_ops.dashboard_name
}
```

### Step 14: Deploy the Dashboard

Initialize and apply the Terraform configuration:

```bash
# Initialize the backend and download providers
terraform init
```

**Expected Output:**

```
Initializing the backend...

Successfully configured the backend "s3"! Terraform will automatically
use this backend unless the backend configuration changes.

Initializing provider plugins...
- Finding hashicorp/aws versions matching "~> 6.0"...
- Installing hashicorp/aws v6.x.x...
- Installed hashicorp/aws v6.x.x (signed by HashiCorp)

Terraform has been successfully initialized!
```

Now review the plan and apply:

```bash
# Review the plan
terraform plan
```

**Expected Output:**

```
Terraform used the selected providers to generate the following execution plan.
Resource actions are indicated with the following symbols:
  + create

Terraform will perform the following actions:

  # aws_cloudwatch_dashboard.terraform_ops will be created
  + resource "aws_cloudwatch_dashboard" "terraform_ops" {
      + dashboard_arn  = (known after apply)
      + dashboard_body = jsonencode(
            {
              + widgets = [
                  + { ... },
                  + { ... },
                  ...
                ]
            }
        )
      + dashboard_name = "studentXX-terraform-operations"
      + id             = (known after apply)
    }

Plan: 1 to add, 0 to change, 0 to destroy.

Changes to Outputs:
  + dashboard_name = "studentXX-terraform-operations"
  + dashboard_url  = "https://console.aws.amazon.com/cloudwatch/home?region=us-east-1#dashboards:name=studentXX-terraform-operations"
```

Apply the configuration:

```bash
terraform apply
```

Type `yes` when prompted.

**Expected Output:**

```
aws_cloudwatch_dashboard.terraform_ops: Creating...
aws_cloudwatch_dashboard.terraform_ops: Creation complete after 1s [id=studentXX-terraform-operations]

Apply complete! Resources: 1 added, 0 changed, 0 destroyed.

Outputs:

dashboard_name = "studentXX-terraform-operations"
dashboard_url = "https://console.aws.amazon.com/cloudwatch/home?region=us-east-1#dashboards:name=studentXX-terraform-operations"
```

### Step 15: Open the Dashboard and Explore

Get the dashboard URL:

```bash
terraform output dashboard_url
```

Open the URL in your browser. You should see the dashboard with the following sections:

**Row 1: Pipeline Overview**
- **CodeBuild Duration** -- time series showing how long each build stage takes. Look for patterns: validate should be fast (10-30s), apply stages take longer.
- **Build Success vs Failure** -- tracks whether staging and production apply stages succeed or fail. A healthy pipeline shows mostly green.
- **Pipeline Succeeded/Failed** -- single-value counters showing total pipeline executions for the day.
- **Pipeline Executions Over Time** -- stacked time series of pipeline runs.

**Row 2: State & Infrastructure Operations**
- **State Bucket Operations** -- S3 Get and Put requests to your state bucket. Every `terraform plan` reads state (Get), every `terraform apply` writes state (Put). Spikes indicate active deployments.
- **State Lock Operations** -- DynamoDB read and write capacity consumed by lock operations. Each `terraform plan` or `apply` acquires and releases a lock.

**Row 3: Quick Links & Audit Reference**
- **Console Quick Links** -- clickable links to CodePipeline, CodeBuild, CloudTrail, S3, DynamoDB, and CloudWatch Logs Insights.
- **Audit Query Reference** -- ready-to-use CloudWatch Log Insights queries that your SOC team can copy and run at any time.

> **Note:** Some widgets may show "No data available" if not enough time has passed for CloudWatch metrics to populate. This is normal -- metrics typically appear within 5-10 minutes of activity. The CodeBuild and CodePipeline metrics from Lab 3 should already be visible.

### Step 16: Discussion -- Extending Observability

The dashboard we deployed provides foundational visibility. In a production environment, NovaTech would extend this with:

**1. SNS Alerts on Pipeline Failures**

```hcl
# Example: Alert when pipeline fails (NOT deployed in this lab)
resource "aws_cloudwatch_metric_alarm" "pipeline_failed" {
  alarm_name          = "${var.student_id}-pipeline-failed"
  comparison_operator = "GreaterThanThreshold"
  evaluation_periods  = 1
  metric_name         = "PipelineExecutionFailed"
  namespace           = "AWS/CodePipeline"
  period              = 300
  statistic           = "Sum"
  threshold           = 0
  alarm_description   = "Alert when Terraform pipeline execution fails"
  alarm_actions       = [aws_sns_topic.alerts.arn]

  dimensions = {
    PipelineName = "${var.student_id}-terraform-pipeline"
  }
}
```

**2. Drift Detection**
- Schedule a Lambda function to run `terraform plan` nightly
- Publish custom CloudWatch metrics for drift count
- Alert if any environment shows unexpected changes

**3. Cost Tracking**
- Tag-based cost allocation reports tied to `Student` and `Environment` tags
- Integration with AWS Cost Explorer API
- Dashboard widget showing monthly infrastructure cost by environment

**4. Compliance Reports**
- Automated weekly exports of CloudTrail data to S3 for long-term retention
- Integration with AWS Config rules to verify resource compliance
- AWS Security Hub findings for infrastructure misconfigurations

---

## Part D: Discussion -- What We Built Today

> **This section ties the entire day together. Take a moment to reflect on how each lab built upon the previous one to create a complete enterprise Terraform workflow.**

### The NovaTech Transformation

At 9:00 AM, NovaTech had individual engineers running `terraform apply` from their laptops against shared AWS accounts. There was no coordination, no audit trail, and significant legacy infrastructure that existed outside of Terraform's management. The compliance team's questions were unanswerable.

At 5:15 PM, NovaTech has:

| Lab | Problem Solved | What We Built | Enterprise Capability |
|-----|---------------|---------------|----------------------|
| **Lab 1: State Management** | Engineers overwriting each other's state files | S3 backend with DynamoDB locking, multi-environment state paths | Team collaboration foundation -- no more state corruption or concurrent modification |
| **Lab 2: Import** | Legacy infrastructure outside Terraform management | Imported a complete application stack (VPC, ALB, ASG) with zero downtime | Bringing legacy under management -- all infrastructure is now code |
| **Lab 3: CI/CD Pipeline** | `terraform apply` from laptops with no review or approval | CodePipeline with validation, planning, approval gates, and environment promotion | Automated, auditable deployments -- no more rogue applies |
| **Lab 4: Observability** | "Who deployed what?" questions were unanswerable | CloudTrail queries, Log Insights, and a CloudWatch operational dashboard | Operational visibility and compliance -- every change is traceable |

### The SOC 2 Readiness Checklist

When the auditor arrives, Jordan can now demonstrate:

- **Change Management:** All infrastructure changes flow through a version-controlled CI/CD pipeline with mandatory approval gates (Lab 3).
- **Audit Trail:** Every AWS API call is captured in CloudTrail with full attribution -- who made the call, when, and through what mechanism (Lab 4, Part A).
- **Access Control:** Infrastructure deployments use a dedicated IAM role with least-privilege permissions, not individual engineer credentials (Lab 3 IAM role).
- **State Integrity:** Terraform state is encrypted at rest, versioned for rollback capability, and protected by DynamoDB locking to prevent corruption (Lab 1).
- **Monitoring & Alerting:** An operational dashboard provides continuous visibility into deployment health, and can be extended with automated alerts (Lab 4, Part C).
- **Separation of Duties:** The approval gate in the pipeline ensures that the person who writes the code is not the same person who approves the production deployment (Lab 3).

> **Key Insight:** None of these capabilities exist in isolation. State management enables the pipeline. The pipeline generates the audit trail. The audit trail feeds the dashboard. Together, they form a complete enterprise workflow.

---

## Troubleshooting

### Issue 1: CloudTrail Events Not Showing

**Symptom:** You navigate to CloudTrail Event History but do not see events from your lab activity.

**Cause:** CloudTrail events can take up to **15 minutes** to appear in the Event History console. Recent events may not yet be visible.

**Resolution:**
1. Wait 10-15 minutes and refresh the page
2. Verify your region is set to **us-east-1** (top-right corner of the console)
3. Expand the time range filter to include the full day
4. Try a broader filter (e.g., Event name = `PutParameter`) before narrowing down

### Issue 2: Dashboard Not Showing Data

**Symptom:** The CloudWatch dashboard is deployed successfully but some or all widgets show "No data available" or "Insufficient data."

**Cause:** CloudWatch metrics need time to populate. CodeBuild and CodePipeline metrics are published after build/pipeline completion. S3 request metrics require a request metrics configuration on the bucket (not enabled by default). DynamoDB metrics for on-demand tables may show zero if no requests have been made recently.

**Resolution:**
1. Wait 5-10 minutes after your most recent pipeline execution and refresh the dashboard
2. Adjust the dashboard time range to **Last 3 hours** or **Last 12 hours** to capture Lab 3 activity
3. For S3 request metrics: these require enabling request metrics on the S3 bucket. The widget will show data once metrics are configured. For this lab, focus on the CodeBuild and CodePipeline widgets.
4. Run a quick `terraform plan` from the `lab4-observability` directory to generate fresh DynamoDB lock activity

### Issue 3: Log Insights Query Returns No Results

**Symptom:** The CloudWatch Log Insights query runs but returns zero results.

**Cause:** CloudTrail may not be configured to deliver events to CloudWatch Logs in your account. The Event History (Part A) uses CloudTrail's built-in 90-day retention, but Log Insights (Part B) requires a separate CloudWatch Logs delivery configuration.

**Resolution:**
1. Verify that a CloudTrail log group exists: navigate to CloudWatch > Log groups and search for "cloudtrail"
2. If no CloudTrail log group exists, your instructor may need to configure CloudTrail to deliver to CloudWatch Logs
3. You can still complete Parts A and C without Log Insights -- Part B is supplementary
4. As an alternative, try querying CodeBuild log groups instead: look for `/aws/codebuild/studentXX-terraform-*` log groups

---

## Knowledge Check

### Question 1

**How can you determine from a CloudTrail event whether a resource change was made through the Terraform CI/CD pipeline or manually from the AWS console?**

<details>
<summary>Click to reveal answer</summary>

Examine three fields in the CloudTrail event:

1. **`sourceIPAddress`** -- Pipeline activity shows `codebuild.amazonaws.com`; console activity shows `console.amazonaws.com` or the user's IP address.
2. **`userAgent`** -- Terraform pipeline activity includes `Terraform/1.5.0` in the user agent string; console activity shows `console.amazonaws.com` or `aws-cli`.
3. **`userIdentity.arn`** -- Pipeline activity shows the CodeBuild service role (`studentXX-codebuild-terraform-role`); console or CLI activity shows the IAM user's ARN.

The combination of all three fields provides definitive proof of the change's origin.

</details>

### Question 2

**A CloudTrail event shows the following: `userIdentity.arn` = `arn:aws:sts::123456789012:assumed-role/studentXX-codebuild-terraform-role/studentXX-terraform-apply-staging`. What information can you extract from this ARN, and how would you trace it back to the person who initiated the change?**

<details>
<summary>Click to reveal answer</summary>

From the ARN, you can extract:

- **IAM Role:** `studentXX-codebuild-terraform-role` -- the IAM role assumed by CodeBuild
- **Session Name:** `studentXX-terraform-apply-staging` -- the CodeBuild project name, identifying the exact pipeline stage (the apply stage for the staging environment)

To trace back to the person who initiated the change:

1. Note the `eventTime` from the CloudTrail event
2. Navigate to **CodePipeline** > click on `studentXX-terraform-pipeline` > view **Execution history**
3. Find the pipeline execution that was running at the matching timestamp
4. The execution details show the **Source** revision (the CodeCommit commit hash)
5. Navigate to **CodeCommit** > find the commit > view the **Author** -- this is the person who wrote and pushed the code that triggered the pipeline

This creates the full chain: Person > Git Commit > Pipeline Execution > CodeBuild Stage > API Call > Resource Change.

</details>

### Question 3

**Why does the observability dashboard include S3 and DynamoDB metrics alongside CodeBuild and CodePipeline metrics? What operational insight does this provide?**

<details>
<summary>Click to reveal answer</summary>

The S3 and DynamoDB metrics track **Terraform state operations**, which provide a different layer of insight than pipeline execution metrics:

- **S3 Get Requests** indicate how often Terraform reads the state file. A spike in Get requests means multiple plans are running, which could indicate active development or troubleshooting.
- **S3 Put Requests** indicate state file writes, which occur after every successful `terraform apply`. Each Put is a confirmed infrastructure change.
- **DynamoDB Read/Write** capacity tracks lock acquisition and release. Unusual patterns (e.g., sustained write activity) could indicate a stuck lock or concurrent modification attempt.

Together, these metrics answer questions that pipeline metrics alone cannot:
- "Is someone running Terraform outside the pipeline?" (unexpected S3 Put requests without matching pipeline executions)
- "Is there a locking problem?" (DynamoDB writes without corresponding releases)
- "How active is our Terraform usage overall?" (S3 Get request frequency)

</details>

### Question 4

**If NovaTech's SOC 2 auditor asks "Can you prove that no infrastructure changes were made outside the approved pipeline in the last 30 days?", what combination of tools and queries would you use to provide that evidence?**

<details>
<summary>Click to reveal answer</summary>

You would use a three-step approach:

**Step 1: Identify all Terraform-initiated changes**
Run a CloudWatch Log Insights query filtering for `userAgent like /Terraform/` over the last 30 days. This captures all Terraform API calls regardless of source.

**Step 2: Verify all Terraform calls came from the pipeline**
Add a filter: `sourceIPAddress = "codebuild.amazonaws.com"`. If this query returns the same count as Step 1, all Terraform activity originated from CodeBuild. If the counts differ, some Terraform activity came from outside the pipeline.

**Step 3: Cross-reference with pipeline execution history**
For additional confidence, export the CodePipeline execution history for the last 30 days and verify that every `terraform apply` event in CloudTrail correlates to a pipeline execution with an approved approval gate.

**Evidence package for the auditor:**
1. Log Insights query results (CSV export) showing all Terraform API calls with `sourceIPAddress = codebuild.amazonaws.com`
2. CodePipeline execution history showing all executions with approval timestamps
3. CloudWatch dashboard screenshot showing pipeline health metrics for the 30-day period

If any manual Terraform activity is detected (sourceIPAddress is not `codebuild.amazonaws.com`), investigate immediately and document whether it was authorized (e.g., emergency break-glass procedure).

</details>

---

## Lab Completion Checklist

Verify that you have completed all of the following:

- [ ] Navigated to CloudTrail Event History in the AWS Console
- [ ] Filtered CloudTrail events by Event name (`PutParameter`, `RunInstances`)
- [ ] Filtered CloudTrail events by User name (`studentXX-codebuild-terraform-role`)
- [ ] Examined a full CloudTrail event JSON and identified the key audit fields (`userIdentity.arn`, `userAgent`, `sourceIPAddress`, `eventTime`)
- [ ] Compared a pipeline-initiated event vs. a console/manual event
- [ ] Navigated to CloudWatch Log Insights
- [ ] Ran the Terraform user-agent activity query (or observed instructor demo)
- [ ] Ran the SSM parameter changes query filtered by student ID (or observed instructor demo)
- [ ] Saved a query for reuse (or observed instructor demo)
- [ ] Created the `lab4-observability` working directory with `providers.tf`, `variables.tf`, and `dashboard.tf`
- [ ] Successfully ran `terraform init` and `terraform apply` to deploy the dashboard
- [ ] Opened the CloudWatch dashboard URL and explored the pipeline, state, and quick links widgets

---

## Cost Considerations

| Resource | Cost | Notes |
|----------|------|-------|
| **CloudWatch Dashboard** | $3.00/month per dashboard | First 3 dashboards are free; each additional dashboard costs $3/month |
| **CloudTrail Management Events** | Free | Management event logging is included at no charge for the first trail |
| **CloudWatch Log Insights** | $0.005 per GB scanned | Queries against CloudTrail logs incur charges based on the volume of log data scanned |
| **S3 State Bucket** | ~$0.023/GB/month + request costs | Minimal for state files (typically < 1 MB); request metrics add negligible cost |
| **DynamoDB Lock Table** | Pay-per-request pricing | Lock operations are infrequent and lightweight; cost is typically < $0.01/month |

**Total estimated cost for this lab: < $5/month** if the dashboard is left running.

---

## Cleanup

When the lab is complete and your instructor confirms it is time to clean up, destroy all resources **in reverse order** to respect dependencies.

### Step 1: Destroy Lab 4 (Observability Dashboard)

```bash
cd ~/terraform-day3/lab4-observability
terraform destroy
```

Type `yes` when prompted.

**Expected Output:**

```
aws_cloudwatch_dashboard.terraform_ops: Destroying... [id=studentXX-terraform-operations]
aws_cloudwatch_dashboard.terraform_ops: Destruction complete after 1s

Destroy complete! Resources: 1 destroyed.
```

### Step 2: Destroy Lab 3 (CI/CD Pipeline)

```bash
cd ~/terraform-day3/lab3-pipeline
terraform destroy
```

Type `yes` when prompted. This destroys the CodePipeline, CodeBuild projects, CodeCommit repository, IAM roles, and artifacts bucket.

> **Note:** If pipeline-deployed resources (staging/prod SSM parameters) were created through the pipeline, you may need to destroy those first by running `terraform destroy` inside the cloned app-repo's staging and prod environment directories before destroying the pipeline itself.

```bash
# If needed: destroy pipeline-deployed resources first
cd ~/terraform-day3/app-repo/environments/staging
terraform destroy

cd ~/terraform-day3/app-repo/environments/prod
terraform destroy

# Then destroy the pipeline
cd ~/terraform-day3/lab3-pipeline
terraform destroy
```

### Step 3: Destroy Lab 2 (Imported Application)

```bash
cd ~/terraform-day3/lab2-import
terraform destroy
```

Type `yes` when prompted. This destroys the imported legacy application stack (VPC, ALB, ASG, subnets, security groups, etc.).

> **Warning:** This will terminate the legacy application. Only proceed if your instructor confirms cleanup.

### Step 4: Destroy Lab 1 (State Infrastructure)

```bash
cd ~/terraform-day3/lab1-state/staging-app-west
terraform destroy

cd ~/terraform-day3/lab1-state/staging-app
terraform destroy
```

For the state backend infrastructure itself (S3 bucket and DynamoDB table), you need to empty the S3 bucket first (because it contains state files from all labs):

```bash
# Empty the state bucket (required before Terraform can delete it)
# Use your actual bucket name from Lab 1 output
aws s3 rm s3://studentXX-terraform-state-SUFFIX --recursive

# Destroy the state infrastructure
# NOTE: This config uses local state, so you can destroy directly
cd ~/terraform-day3/lab1-state
terraform destroy
```

Type `yes` when prompted.

**Expected Output:**

```
aws_s3_bucket_public_access_block.terraform_state: Destroying...
aws_s3_bucket_versioning.terraform_state: Destroying...
aws_s3_bucket_server_side_encryption_configuration.terraform_state: Destroying...
aws_dynamodb_table.terraform_lock: Destroying...
...

Destroy complete! Resources: 5 destroyed.
```

### Cleanup Verification

Confirm all resources are removed:

```bash
# Verify S3 bucket is gone (use your actual bucket name)
aws s3 ls s3://studentXX-terraform-state-SUFFIX 2>&1 | head -1
# Expected: An error occurred (NoSuchBucket)

# Verify DynamoDB table is gone (use your actual table name)
aws dynamodb describe-table --table-name studentXX-terraform-lock-SUFFIX 2>&1 | head -1
# Expected: An error occurred (ResourceNotFoundException)

# Verify dashboard is gone
aws cloudwatch get-dashboard --dashboard-name studentXX-terraform-operations 2>&1 | head -1
# Expected: An error occurred (ResourceNotFound)
```

---

## Next Steps

### Immediate Next Steps

1. **Wrap-up Session (5:15-5:30)** -- Join the final discussion, Q&A, and review of additional learning resources.
2. **Review Your Notes** -- Revisit the CloudTrail event JSON structure and the dashboard HCL. These patterns are directly applicable to your production environments.
3. **Share the Dashboard Pattern** -- The `dashboard.tf` you created today is a reusable template. Adapt it for your organization's specific pipelines and metrics.

### Beyond This Course

| Topic | Description | Where to Learn |
|-------|-------------|----------------|
| **Terraform Cloud / Enterprise** | Managed state, remote execution, policy-as-code, team management | [developer.hashicorp.com/terraform/cloud-docs](https://developer.hashicorp.com/terraform/cloud-docs) |
| **Sentinel / OPA Policy-as-Code** | Enforce compliance rules directly in the Terraform workflow | [developer.hashicorp.com/sentinel](https://developer.hashicorp.com/sentinel) |
| **Module Registry** | Publish and version shared Terraform modules for your organization | [developer.hashicorp.com/terraform/registry](https://developer.hashicorp.com/terraform/registry) |
| **Drift Detection Automation** | Scheduled `terraform plan` with alerting on unexpected changes | AWS Lambda + EventBridge + Custom CloudWatch Metrics |
| **AWS Config Rules** | Continuous compliance monitoring for AWS resource configurations | [docs.aws.amazon.com/config](https://docs.aws.amazon.com/config/latest/developerguide/WhatIsConfig.html) |
| **AWS Security Hub** | Centralized security findings across AWS services | [docs.aws.amazon.com/securityhub](https://docs.aws.amazon.com/securityhub/latest/userguide/what-is-securityhub.html) |

---

## Additional Resources

### Official Documentation

- [AWS CloudTrail User Guide](https://docs.aws.amazon.com/awscloudtrail/latest/userguide/cloudtrail-user-guide.html) -- *Source: AWS*
- [Amazon CloudWatch Dashboards](https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/CloudWatch_Dashboards.html) -- *Source: AWS*
- [CloudWatch Logs Insights Query Syntax](https://docs.aws.amazon.com/AmazonCloudWatch/latest/logs/CWL_QuerySyntax.html) -- *Source: AWS*
- [Terraform aws_cloudwatch_dashboard Resource](https://registry.terraform.io/providers/hashicorp/aws/latest/docs/resources/cloudwatch_dashboard) -- *Source: HashiCorp*
- [CloudWatch Dashboard Body Structure](https://docs.aws.amazon.com/AmazonCloudWatch/latest/APIReference/CloudWatch-Dashboard-Body-Structure.html) -- *Source: AWS*
- [AWS CodePipeline Metrics in CloudWatch](https://docs.aws.amazon.com/codepipeline/latest/userguide/monitoring-cloudwatch.html) -- *Source: AWS*

### SOC 2 and Compliance

- [AWS SOC 2 Compliance](https://aws.amazon.com/compliance/soc-faqs/) -- *Source: AWS*
- [AWS Well-Architected Framework: Operational Excellence Pillar](https://docs.aws.amazon.com/wellarchitected/latest/operational-excellence-pillar/welcome.html) -- *Source: AWS*
- [Terraform Best Practices for Enterprise](https://developer.hashicorp.com/terraform/cloud-docs/recommended-practices) -- *Source: HashiCorp*

---

*Terraform Day 3: Enterprise Deployment & Operations -- Lab 4: Auditing & Observability*
*Course Creation Framework v2.0 | Level 300: Advanced Production Operations*
